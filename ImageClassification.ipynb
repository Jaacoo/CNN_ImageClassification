{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fMrm_4lQsbfXNRRGy_1Kx274_hqPxnZb",
      "authorship_tag": "ABX9TyM5FrGKENwR3SG+v7pxHZqv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaacoo/CNN_ImageClassification/blob/master/ImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xC1aiYwa7Hq",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJOD_b0mbI6Q",
        "colab_type": "text"
      },
      "source": [
        "In this project we are going to use Tensorflow Keras and a Convolutional Neural Network (CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E0whwlVky6w",
        "colab_type": "text"
      },
      "source": [
        "Before we start, we need to import several libraries and modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy2tQ3H3bXaJ",
        "colab_type": "code",
        "outputId": "9f7b79e3-3a10-4aaf-82f1-e74b81efcd84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#numpy is a library for scientific calculation. It consists of multidimensional array objects known as ndarray objects\n",
        "import numpy as np \n",
        "# OpenCV (here imported as cv2) is a library designed to solve computer vision problems\n",
        "import cv2\n",
        "# This module is for using operating system dependent functionality\n",
        "import os\n",
        "\n",
        "#importing TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import utils\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhQOa2Lq4PeJ",
        "colab_type": "text"
      },
      "source": [
        "In this step we are going to prepare our data. We are extracting the images and labeling them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2hOp7MKcXks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In 'data_path' all our data / images are stored \n",
        "def extract_imgs(data_path, count):\n",
        "  image_list = []\n",
        "  label_list = []\n",
        "  lab = 0\n",
        "    \n",
        "  #iterating through the folder structure\n",
        "  for label in os.listdir(data_path):\n",
        "    if label == 'buildings': \n",
        "      lab = 0\n",
        "    elif label == 'forest':\n",
        "      lab = 1\n",
        "    elif label == 'glacier':\n",
        "      lab = 2\n",
        "    elif label == 'mountain':\n",
        "      lab = 3\n",
        "    elif label == 'sea':\n",
        "      lab = 4\n",
        "    elif label == 'street':\n",
        "      lab = 5\n",
        "    # iterate through the above folder structure and take an image from current \n",
        "    # folder and resize it (images have different sizes in the data set)\n",
        "    # at the end append the current image and the corresponding label\n",
        "    i = 0\n",
        "    for image_file in os.listdir(data_path+label): # Extracting the file name of the image from Class Label folder\n",
        "      image = cv2.imread(data_path+label+r'/'+image_file) # Reading the image (OpenCV)\n",
        "      image = cv2.resize(image,(256,256)) # Resize the images to 256x256\n",
        "      image_list.append(image)\n",
        "      label_list.append(lab)\n",
        "      i=i+1\n",
        "      # If only 'count' images are required\n",
        "      if i == count:\n",
        "        i = 0\n",
        "        break\n",
        "  return (np.array(image_list), np.array(label_list))\n",
        "\n",
        "#shuffling ordered data\n",
        "def shuffle_data(images, labels):\n",
        "  return shuffle(images, labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJdd1QoH8TqQ",
        "colab_type": "text"
      },
      "source": [
        "Extracting data from folders and exploring the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdnnLVqBbADI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_raw, y_train_raw = extract_imgs('/content/drive/My Drive/ImageClassification/seg_train/',2100)\n",
        "X_train, y_train = shuffle_data(X_train_raw, y_train_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHN9AcUKPR8c",
        "colab_type": "text"
      },
      "source": [
        "In this step we are going to build the model. It is going to be a CNN with several convolution and max pooling layers.\n",
        "With Batch Normalization we normalize the activations of the previous layer at each batch.\n",
        "\n",
        "After Convolution and Max Pool layers we are going to add fully connected layers. In order to avoid overfitting we are going to use \"Drop out\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TXd64qCSIyM",
        "colab_type": "code",
        "outputId": "0f07e90e-0eae-478e-a3d3-42261fa168eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "#1st Convolutional Layer\n",
        "model.add(layers.Conv2D(input_shape=(256,256,3), filters=96, kernel_size=(11,11), strides=(4,4), padding='valid', activation='relu'))\n",
        "#Max Pooling\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "model.add(layers.Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid', activation='relu'))\n",
        "#Max Pooling\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "#3rd Convolutional Layer\n",
        "model.add(layers.Conv2D(filters=96, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "#Max Pooling\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(2048, activation='relu'))\n",
        "model.add(layers.Dropout(rate=0.5))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dropout(rate=0.5))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(6, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 62, 62, 96)        34944     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 31, 31, 96)        384       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 21, 21, 256)       2973952   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10, 10, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 96)          221280    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4, 4, 96)          384       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1536)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2048)              3147776   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 2048)              8192      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 7,442,918\n",
            "Trainable params: 7,436,518\n",
            "Non-trainable params: 6,400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fEQBcDuvtI_",
        "colab_type": "text"
      },
      "source": [
        "In the next step we are going to compile the model. We are using the Adam optimizer. The loss function is \"Sparse Categorical Crossentropy\" as we predict only integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgrjWEFRY-Py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZwNAI50YyxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fitting the model with our train data\n",
        "model_trained = model.fit(X_train,y_train,epochs=20,validation_split=0.30, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8Yy24OnDh69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extracting test data\n",
        "X_test, y_test = extract_imgs('/content/drive/My Drive/ImageClassification/seg_test/',100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVrzb5iXFO22",
        "colab_type": "code",
        "outputId": "78400f75-34bf-405f-8649-e5e0ba5fbb77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#evaluating the model with the test data\n",
        "model.evaluate(X_test, y_test, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 - 0s - loss: 1.0790 - accuracy: 0.7733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0789562463760376, 0.7733333110809326]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql5dzIrqJFCC",
        "colab_type": "text"
      },
      "source": [
        "As we can see our model has an accuracy of 77,3% on our test data."
      ]
    }
  ]
}